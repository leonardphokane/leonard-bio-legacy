{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea8fd0d",
   "metadata": {},
   "source": [
    "# üîç AI Ethics: Fairness Audit of COMPAS Recidivism Data\n",
    "\n",
    "This notebook presents a bias audit using IBM's **AI Fairness 360** toolkit on the widely studied **COMPAS Recidivism dataset**. The goal is to assess whether the algorithm exhibits racial bias in predicting recidivism risk scores, with a particular focus on disparities between **African-American** and **Caucasian** defendants.\n",
    "\n",
    "### üåç Why This Matters\n",
    "Ethical AI systems must ensure fairness, transparency, and accountability‚Äîespecially in sensitive domains like criminal justice. The COMPAS dataset has been scrutinized for producing unequal outcomes that may contribute to systemic injustices.\n",
    "\n",
    "### ‚öôÔ∏è Objectives\n",
    "- Load and explore the COMPAS dataset  \n",
    "- Define protected attributes and audit groups  \n",
    "- Calculate fairness metrics such as **Disparate Impact** and **False Positive Rate**  \n",
    "- Visualize racial disparities in model predictions  \n",
    "- Summarize findings and propose mitigation strategies\n",
    "\n",
    "### üìö Tools & Libraries\n",
    "- `AI Fairness 360`  \n",
    "- `Pandas`  \n",
    "- `Matplotlib`  \n",
    "- `Jupyter Notebook`\n",
    "\n",
    "Let‚Äôs uncover how data shapes decisions‚Äîand how fairness can reshape the future. ‚öñÔ∏è‚ú®\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c179583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from aif360.datasets import CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44bb1a4",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Dataset Overview: COMPAS Recidivism Predictions\n",
    "\n",
    "This audit uses the **COMPAS Recidivism dataset**, which contains criminal justice records used to predict whether a defendant is likely to reoffend. The dataset includes features such as:\n",
    "\n",
    "- Demographics (e.g. race, age, gender)\n",
    "- Criminal history (e.g. prior arrests, charge types)\n",
    "- Risk assessment scores generated by COMPAS\n",
    "\n",
    "### üîê Protected Attribute\n",
    "The **race** attribute is used as a protected variable for this analysis. We'll examine how predictions differ between:\n",
    "\n",
    "- **Privileged group**: Caucasian defendants\n",
    "- **Unprivileged group**: African-American defendants\n",
    "\n",
    "### ‚ö†Ô∏è Known Concerns\n",
    "Prior investigations (e.g., by ProPublica) highlighted that African-American defendants may be disproportionately labeled high-risk, even when actual outcomes do not justify the prediction. This raises serious fairness concerns.\n",
    "\n",
    "The goal of this notebook is to **quantify racial disparities** in the model's predictions using well-established metrics and visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf96b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CompasDataset()\n",
    "\n",
    "# View data shape and labels\n",
    "print(\"Features shape:\", dataset.features.shape)\n",
    "print(\"Label names:\", dataset.label_names)\n",
    "print(\"Protected attributes:\", dataset.protected_attribute_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87871f40",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Defining Protected Groups for Bias Analysis\n",
    "\n",
    "To assess fairness, we categorize individuals based on their **race**‚Äîa protected attribute in the COMPAS dataset.\n",
    "\n",
    "- **Privileged Group**: Caucasian defendants (`race = 1`)\n",
    "- **Unprivileged Group**: African-American defendants (`race = 0`)\n",
    "\n",
    "By comparing outcomes across these groups, we can measure if the algorithm unfairly favors or penalizes one over the other.\n",
    "\n",
    "These definitions guide all subsequent fairness metrics, including disparate impact and classification errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d17f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'race': 1}]       # Caucasian\n",
    "unprivileged_groups = [{'race': 0}]     # African-American\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd6639",
   "metadata": {},
   "source": [
    "## üìä Metric 1: Disparate Impact Ratio\n",
    "\n",
    "The **Disparate Impact Ratio** compares the rate of favorable outcomes between unprivileged and privileged groups.\n",
    "\n",
    "- A ratio of **1.0** indicates perfect parity.\n",
    "- A ratio **below 0.8** is generally considered biased.\n",
    "\n",
    "This metric helps identify systemic inequalities in decision outcomes, serving as a diagnostic starting point for auditing algorithmic fairness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d11bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = BinaryLabelDatasetMetric(dataset,\n",
    "                                  privileged_groups=privileged_groups,\n",
    "                                  unprivileged_groups=unprivileged_groups)\n",
    "\n",
    "print(\"Disparate Impact Ratio:\", metric.disparate_impact())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a6567",
   "metadata": {},
   "source": [
    "## üìâ Metric 2: False Positive Rate by Race\n",
    "\n",
    "We examine the **False Positive Rate (FPR)** to identify how often each group is incorrectly labeled as high-risk:\n",
    "\n",
    "- A higher FPR for African-American defendants suggests a **racial disparity**, with more individuals flagged unfairly.\n",
    "- This visualization brings clarity to the audit by showing concrete predictive errors that may result in real-world harm.\n",
    "\n",
    "Visual evidence strengthens our findings and guides remediation strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7745fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_metric = ClassificationMetric(dataset, dataset,\n",
    "                                  unprivileged_groups=unprivileged_groups,\n",
    "                                  privileged_groups=privileged_groups)\n",
    "\n",
    "fp_unpriv = classification_metric.false_positive_rate(privileged=False)\n",
    "fp_priv = classification_metric.false_positive_rate(privileged=True)\n",
    "\n",
    "plt.bar(['African-American', 'Caucasian'], [fp_unpriv, fp_priv], color=['red', 'blue'])\n",
    "plt.title('False Positive Rate by Race')\n",
    "plt.ylabel('Rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e2df4",
   "metadata": {},
   "source": [
    "## üßæ Summary: Interpreting Fairness Metrics\n",
    "\n",
    "Our fairness audit of the COMPAS Recidivism dataset reveals measurable racial disparities:\n",
    "\n",
    "- The **Disparate Impact Ratio** falls below the acceptable threshold of 0.8, indicating unequal favorable outcomes.\n",
    "- The **False Positive Rate** for African-American defendants is notably higher than that of Caucasian defendants, suggesting that the algorithm more often misclassifies them as high-risk.\n",
    "\n",
    "These results align with prior critiques of COMPAS and underscore the importance of auditing AI systems in high-stakes scenarios.\n",
    "\n",
    "### ‚úçÔ∏è Next Steps\n",
    "To address the issues revealed:\n",
    "- Consider fairness-aware preprocessing like **reweighing**.\n",
    "- Explore bias mitigation algorithms and retraining.\n",
    "- Document the ethical implications and recommendations clearly in your report.\n",
    "\n",
    "Responsible AI begins with transparency‚Äîand evolves through fairness in action.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
